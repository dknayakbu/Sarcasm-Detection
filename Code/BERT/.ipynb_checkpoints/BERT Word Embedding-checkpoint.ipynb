{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw=pd.read_json(r\"./../../Dataset/Sarcasm_Headlines_Dataset_v2/Sarcasm_Headlines_Dataset_v2.json\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_raw.loc[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28619, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['is_sarcastic', 'headline', 'article_link'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_sarcastic                                           headline  \\\n",
       "0             1  thirtysomething scientists unveil doomsday clo...   \n",
       "1             0  dem rep. totally nails why congress is falling...   \n",
       "2             0  eat your veggies: 9 deliciously different recipes   \n",
       "3             1  inclement weather prevents liar from getting t...   \n",
       "4             1  mother comes pretty close to using word 'strea...   \n",
       "\n",
       "                                        article_link  \n",
       "0  https://www.theonion.com/thirtysomething-scien...  \n",
       "1  https://www.huffingtonpost.com/entry/donna-edw...  \n",
       "2  https://www.huffingtonpost.com/entry/eat-your-...  \n",
       "3  https://local.theonion.com/inclement-weather-p...  \n",
       "4  https://www.theonion.com/mother-comes-pretty-c...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Distil-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## For BERT Base, uncomment the following line:\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = df['headline'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28619, 193)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28619, 193)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28619, 193])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor(padded)\n",
    "input_ids = input_ids.type(torch.LongTensor)\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28619, 193])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = torch.tensor(attention_mask)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# with torch.no_grad():\n",
    "#     last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = attention_mask.to('cuda')\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28619, 193])\n",
      "torch.Size([28619, 193])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size = 250\n",
      "Number of Batches = 115\n",
      "Reminder in last batch = 119\n"
     ]
    }
   ],
   "source": [
    "batch_size = 250\n",
    "no_of_batches=(df.shape[0]//batch_size)+1\n",
    "reminder=(df.shape[0]%batch_size)\n",
    "print(\"Batch Size =\",batch_size)\n",
    "print(\"Number of Batches =\",no_of_batches)\n",
    "print(\"Reminder in last batch =\",reminder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([250, 193])\n",
      "1 torch.Size([250, 193])\n",
      "2 torch.Size([250, 193])\n",
      "3 torch.Size([250, 193])\n",
      "4 torch.Size([250, 193])\n",
      "5 torch.Size([250, 193])\n",
      "6 torch.Size([250, 193])\n",
      "7 torch.Size([250, 193])\n",
      "8 torch.Size([250, 193])\n",
      "9 torch.Size([250, 193])\n",
      "10 torch.Size([250, 193])\n",
      "11 torch.Size([250, 193])\n",
      "12 torch.Size([250, 193])\n",
      "13 torch.Size([250, 193])\n",
      "14 torch.Size([250, 193])\n",
      "15 torch.Size([250, 193])\n",
      "16 torch.Size([250, 193])\n",
      "17 torch.Size([250, 193])\n",
      "18 torch.Size([250, 193])\n",
      "19 torch.Size([250, 193])\n",
      "20 torch.Size([250, 193])\n",
      "21 torch.Size([250, 193])\n",
      "22 torch.Size([250, 193])\n",
      "23 torch.Size([250, 193])\n",
      "24 torch.Size([250, 193])\n",
      "25 torch.Size([250, 193])\n",
      "26 torch.Size([250, 193])\n",
      "27 torch.Size([250, 193])\n",
      "28 torch.Size([250, 193])\n",
      "29 torch.Size([250, 193])\n",
      "30 torch.Size([250, 193])\n",
      "31 torch.Size([250, 193])\n",
      "32 torch.Size([250, 193])\n",
      "33 torch.Size([250, 193])\n",
      "34 torch.Size([250, 193])\n",
      "35 torch.Size([250, 193])\n",
      "36 torch.Size([250, 193])\n",
      "37 torch.Size([250, 193])\n",
      "38 torch.Size([250, 193])\n",
      "39 torch.Size([250, 193])\n",
      "40 torch.Size([250, 193])\n",
      "41 torch.Size([250, 193])\n",
      "42 torch.Size([250, 193])\n",
      "43 torch.Size([250, 193])\n",
      "44 torch.Size([250, 193])\n",
      "45 torch.Size([250, 193])\n",
      "46 torch.Size([250, 193])\n",
      "47 torch.Size([250, 193])\n",
      "48 torch.Size([250, 193])\n",
      "49 torch.Size([250, 193])\n",
      "50 torch.Size([250, 193])\n",
      "51 torch.Size([250, 193])\n",
      "52 torch.Size([250, 193])\n",
      "53 torch.Size([250, 193])\n",
      "54 torch.Size([250, 193])\n",
      "55 torch.Size([250, 193])\n",
      "56 torch.Size([250, 193])\n",
      "57 torch.Size([250, 193])\n",
      "58 torch.Size([250, 193])\n",
      "59 torch.Size([250, 193])\n",
      "60 torch.Size([250, 193])\n",
      "61 torch.Size([250, 193])\n",
      "62 torch.Size([250, 193])\n",
      "63 torch.Size([250, 193])\n",
      "64 torch.Size([250, 193])\n",
      "65 torch.Size([250, 193])\n",
      "66 torch.Size([250, 193])\n",
      "67 torch.Size([250, 193])\n",
      "68 torch.Size([250, 193])\n",
      "69 torch.Size([250, 193])\n",
      "70 torch.Size([250, 193])\n",
      "71 torch.Size([250, 193])\n",
      "72 torch.Size([250, 193])\n",
      "73 torch.Size([250, 193])\n",
      "74 torch.Size([250, 193])\n",
      "75 torch.Size([250, 193])\n",
      "76 torch.Size([250, 193])\n",
      "77 torch.Size([250, 193])\n",
      "78 torch.Size([250, 193])\n",
      "79 torch.Size([250, 193])\n",
      "80 torch.Size([250, 193])\n",
      "81 torch.Size([250, 193])\n",
      "82 torch.Size([250, 193])\n",
      "83 torch.Size([250, 193])\n",
      "84 torch.Size([250, 193])\n",
      "85 torch.Size([250, 193])\n",
      "86 torch.Size([250, 193])\n",
      "87 torch.Size([250, 193])\n",
      "88 torch.Size([250, 193])\n",
      "89 torch.Size([250, 193])\n",
      "90 torch.Size([250, 193])\n",
      "91 torch.Size([250, 193])\n",
      "92 torch.Size([250, 193])\n",
      "93 torch.Size([250, 193])\n",
      "94 torch.Size([250, 193])\n",
      "95 torch.Size([250, 193])\n",
      "96 torch.Size([250, 193])\n",
      "97 torch.Size([250, 193])\n",
      "98 torch.Size([250, 193])\n",
      "99 torch.Size([250, 193])\n",
      "100 torch.Size([250, 193])\n",
      "101 torch.Size([250, 193])\n",
      "102 torch.Size([250, 193])\n",
      "103 torch.Size([250, 193])\n",
      "104 torch.Size([250, 193])\n",
      "105 torch.Size([250, 193])\n",
      "106 torch.Size([250, 193])\n",
      "107 torch.Size([250, 193])\n",
      "108 torch.Size([250, 193])\n",
      "109 torch.Size([250, 193])\n",
      "110 torch.Size([250, 193])\n",
      "111 torch.Size([250, 193])\n",
      "112 torch.Size([250, 193])\n",
      "113 torch.Size([250, 193])\n",
      "114 torch.Size([119, 193])\n",
      "Wall time: 8min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings=np.empty((0, 768), int)\n",
    "for i in range(0,no_of_batches):\n",
    "    a=batch_size*i\n",
    "    input_ids_batch=input_ids[a:a+batch_size]\n",
    "    attention_mask_batch=attention_mask[a:a+batch_size]\n",
    "    print(i,input_ids_batch.shape)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids_batch, attention_mask=attention_mask_batch)\n",
    "        features = last_hidden_states[0][:,0,:].cpu().numpy()\n",
    "        embeddings=np.append(embeddings,features,axis=0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import save\n",
    "# save to npy file\n",
    "save('embeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28619, 768)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.08782839  0.06258331 -0.12000587 ... -0.33156553  0.5002954\n",
      "   0.1805836 ]\n",
      " [-0.43426287 -0.50481713 -0.36135322 ... -0.02944355  0.34979802\n",
      "   0.26927984]\n",
      " [-0.13752083 -0.09303751  0.14121507 ... -0.0079867   0.26747227\n",
      "   0.29764438]\n",
      " ...\n",
      " [-0.28889391 -0.20626098  0.15944281 ... -0.02881556  0.36084583\n",
      "   0.32674789]\n",
      " [-0.66664398 -0.10732649 -0.48685697 ... -0.26801217  0.5380348\n",
      "   0.31238717]\n",
      " [-0.05215537  0.08621484  0.14538945 ... -0.13530289  0.40199804\n",
      "   0.26933154]]\n"
     ]
    }
   ],
   "source": [
    "# load numpy array from npy file\n",
    "from numpy import load\n",
    "# load array\n",
    "embeddings_npy = load('embeddings.npy')\n",
    "# print the array\n",
    "print(embeddings_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
